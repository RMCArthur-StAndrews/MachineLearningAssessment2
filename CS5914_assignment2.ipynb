{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a705a7",
   "metadata": {},
   "source": [
    "# CS5914 Machine Learning Algorithms\n",
    "## Assignment 2 \n",
    "##### Credits: 35% of courework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26dde6",
   "metadata": {},
   "source": [
    "## Aims\n",
    "\n",
    "The objectives of this assignment are:\n",
    "\n",
    "* deepen your understanding of Bayesian inference\n",
    "* deepen your understanding of clustering and unsupervised learning\n",
    "* gain experience in implementing sampling-based Bayesian inference algorithms\n",
    "* gain experience in implementing clustering algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662dc6a",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "Load required packages (you can only use the imported packages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f3a91",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# if you use jupyter-lab, switch to %matplotlib inline instead\n",
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "%config Completer.use_jedi = False\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logsumexp\n",
    "import numpy.linalg as linalg\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578545db-3f07-42f4-9429-393bb58b4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fixed random number generator seed to have reproducible results\n",
    "random_seed = 123\n",
    "rng = np.random.default_rng(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13ff12-10f8-47d6-8863-1aac23b300ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1. Spherical K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b6254-0e5e-4df3-8fe3-c8af8cfb6e9c",
   "metadata": {},
   "source": [
    "### Read-in datasets\n",
    "We are going to use two datasets, `dataset1` and `dataset2`. Read in dataset1:\n",
    "* ``dataset1``: 500 observations and each with 4 features\n",
    "* note that no cluster labels given for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d631719-51fc-41a9-88bc-b1e6adce25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset1\n",
    "d1_df = pd.read_csv('./datasets/dataset1.csv', header=None)\n",
    "dataset1 = np.array(d1_df)\n",
    "# it should contain 500 observations and each with 4 dimensional input\n",
    "n_dataset1, d_dataset1 = dataset1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f73373-8bd2-4cdf-a44b-ff1c7156b51d",
   "metadata": {},
   "source": [
    "Read in dataset2, which contains \n",
    "* ``dataset2``: 500 observations; and each with 3000 features\n",
    "* ``dataset2_labels``: the labels are listed in the last column; there are $K=4$ clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bb668-9532-4558-8c84-efbccfa565dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset2\n",
    "d2_df = pd.read_csv('./datasets/dataset2.csv', header=None)\n",
    "dataset2, dataset2_labels = d2_df.iloc[:, 0:-1], d2_df.iloc[:, -1]\n",
    "dataset2 = np.array(dataset2)\n",
    "dataset2_labels = np.array(dataset2_labels).astype(int)\n",
    "n_dataset2, d_dataset2 = dataset2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64620061-675a-408b-91bb-aa4427d91588",
   "metadata": {},
   "source": [
    "A quick demonstration of clustering performance evaluation\n",
    "* use Sklearn.KMeans to fit a basic clustering model and \n",
    "* evaluate the clustering performance against the true label with normalised mutual information (NMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5b58b-c5a3-4652-88fb-f60c57aae893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate clustering performance by normalised mutual information.\n",
    "# use Kmeans to learn the clusters: use the first four rows as centroids\n",
    "km_dataset2 = KMeans(n_clusters=4, init=dataset2[0:4,:]).fit(dataset2)\n",
    "# should expect approximately < 0.35 performance by using Kmeans\n",
    "normalized_mutual_info_score(km_dataset2.labels_, dataset2_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10d839-674a-46bc-989c-7acf879b99a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spherical K-means\n",
    "The first task is to implement a variant of K-means algorithm, which is called *Spherical K-means*. The algorithm follows a similar procedure as K-means but with slightly different assignment and update steps. In the following, we will explain the spherical K-means algorithm. \n",
    "\n",
    "\n",
    "**Initialisation step**: Start with randomly selecting $K$ data points as the centroid of $K$ clusters. \n",
    "\n",
    "**Assignment step**: *Spherical K-means* assigns a data point to the closest centroid based on *cosine distance* rather than Euclidean distance; specifically, for $i=1,\\ldots, n$\n",
    "\n",
    "$$z^{(i)} \\leftarrow \\arg\\min_{k} \\left (1- \\frac{\\boldsymbol{\\mu}_k^\\top \\mathbf{x}^{(i)} }{\\|\\boldsymbol{\\mu}_k\\| \\cdot \\|\\mathbf{x}^{(i)}\\|}\\right ),$$ where $\\boldsymbol{\\mu}_k^\\top \\mathbf{x}^{(i)} = \\sum_{j=1}^d \\boldsymbol{\\mu}_{kj} \\cdot \\mathbf{x}^{(i)}_{j}$ denotes the inner product and $\\|\\mathbf{x}\\|$ is $L_2$ norm of a vector $\\mathbf{x}$: $\\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x}^\\top \\mathbf{x}}$.\n",
    "\n",
    "**Update step**: *Spherical K-means* updates the centroids such that they are unit one vectors; for $k=1,\\ldots, K$\n",
    "\n",
    "$$\\boldsymbol{\\mu}_k \\leftarrow \\frac{\\sum_{i=1}^n I(z^{(i)} =k) \\cdot  \\mathbf{x}^{(i)}}{\\|\\sum_{i=1}^n I(z^{(i)} =k) \\cdot \\mathbf{x}^{(i)}\\|}.$$ Note that after the normalisation step, the centroids $\\boldsymbol{\\mu}_k$ are norm-one vectors: i.e. $\\|\\boldsymbol{\\mu}_k\\| = 1$ for $k=1,\\ldots, K$.\n",
    "\n",
    "**Repeat** the above two steps **until** the total cosine distance loss converges, where the loss is defined as\n",
    "$$\\texttt{loss} = \\sum_{i=1}^n \\left (1- \\frac{\\boldsymbol{\\mu}_{z^{(i)}}^\\top \\mathbf{x}^{(i)} }{\\|\\boldsymbol{\\mu}_{z^{(i)}}\\| \\cdot \\|\\mathbf{x}^{(i)}\\|}\\right ).$$\n",
    "\n",
    "\n",
    "\n",
    "#### Task 1.1 Implementation of Spherical K-means\n",
    "\n",
    "Your task is to complete the code for *assignment* and *update* steps at the given code template of `sphericalKmeans`.\n",
    "\n",
    "The method `sphericalKmeans` has\n",
    "\n",
    "**Inputs**:\n",
    "* `data`: a $n\\times d$ matrix to cluster, i.e. each row of $\\texttt{data}$ is one observation $x_i$\n",
    "* `K`: the number of the clusters\n",
    "* `tol`: tolerence of error, which is used to check whether the loss has converged so the iteration can stop\n",
    "* `maxIters`: the maximum number of iterations that is allowed\n",
    "\n",
    "**Outputs**:\n",
    "* `losses`: the whole trajectory of losses over the iterations\n",
    "* `zs`: the clustering labels\n",
    "* `us`: the learnt $K$ centroids\n",
    "\n",
    "Feel free to write extra helper methods to make your implementation modularised. You may also rewrite the method altogether as long as your method respect the given mtehod's input/output signature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2612d-1ba4-40d5-a725-1110552ea930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sphericalKmeans(data, K=3, tol= 1e-4, maxIters= 100):\n",
    "    n, d = data.shape \n",
    "    losses = []\n",
    "    # initialisation: randomly assign K observations as centroids\n",
    "    # feel free to use a different but sensible initialisation method    \n",
    "    init_us_ids = rng.integers(n, size = K)\n",
    "    us = data[init_us_ids, :]\n",
    "    zs = np.zeros(n)\n",
    "    # loop until converge \n",
    "    # for i in range(maxIters):\n",
    "        # assignment step\n",
    "\n",
    "        # update step\n",
    "        # convergence check        \n",
    "    return losses, zs, us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a89b70-2ed1-4614-a49f-d76ec7d2e11a",
   "metadata": {},
   "source": [
    "#### Task 1.2 Evaluation\n",
    "\n",
    "Run your implemented algorithm on `dataset1` with $K =3$. Note that like K-means, Spherical K-means also suffers from bad initialisations. To deal with that, we usually run the algorithm multiple times with different random initialisations. To make your life easier, you may want to write a wrapper method that does it automatically.\n",
    "\n",
    "Please report the following information based on your results: \n",
    "* the learned 3 centroids\n",
    "* and also plot the loss trajectory.\n",
    "\n",
    "If you run multiple times, you only need to report the results for the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279265fe-7eeb-466b-ab94-09e121def164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spherical K-means on dataset1 and report your results here\n",
    "# use more cells if you need to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c828f-ec37-43d3-b323-9a29b749e624",
   "metadata": {},
   "source": [
    "Now run your implemented algorithm on `dataset2` with $K =4$. And report the normalised mutual information between the returned cluster labels and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spherical K-means on dataset2 and report your NMI results here\n",
    "# use more cells if you need to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a049f26-7175-43d6-82fb-defafb4d830d",
   "metadata": {},
   "source": [
    "## Task 2. Bayesian Machine Learning\n",
    "\n",
    "\n",
    "In this task, we are going to implement a simple MCMC algorithm to do Bayesian logistic regression. You will be guided to finish the task step by step. First, we read-in the dataset required for the task.\n",
    "\n",
    "### Dataset\n",
    "We are going to use the following simple dataset for this task. The dataset is read-in below and stored in variables `XX` and `yy`. There are in total 16 training instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c029c5-826e-4a99-a73a-766e84455ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3_df = pd.read_csv('./datasets/dataset3.csv', header=0)\n",
    "dataset3 = np.array(d3_df)\n",
    "XX, yy = dataset3[:, 0:2], dataset3[:, -1]\n",
    "XX = np.column_stack((np.ones(XX.shape[0]), XX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a027207-0b43-449e-8286-ddd884a2d0cb",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "The log-likelihood for the probabilistic logistic regression is defined as \n",
    "\n",
    "$$p(y^{(i)}|\\mathbf{w}, \\mathbf{x}^{(i)}) = (\\sigma^{(i)})^{y^{(i)}}(1-\\sigma^{(i)})^{1-y^{(i)}},$$\n",
    "\n",
    "where $\\sigma^{(i)}(\\mathbf{x}, \\mathbf{w}) = \\frac{1}{1+e^{-\\mathbf{w}^\\top\\mathbf{x}}}$ is the logistic regression's output. For simplicity, we assume the feature $\\mathbf{x}$ vector includes dummy one and the weight parameter includes the bias.\n",
    "\n",
    "Give the expression for the joint log-likelihood $\\ln p(\\{y^{(i)}\\}|\\mathbf{w}, \\mathbf{X})$ for the probabilistic model, where $\\mathbf{X}$ is the design matrix ($\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}^{(1)}& \\mathbf{x}^{(2)} & \\ldots & \\mathbf{x}^{(n)}\\end{bmatrix}^\\top$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80fcac-aacb-4a96-ab72-789768f89bf0",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c4487-abc9-4974-bfbe-cd40d09e3c82",
   "metadata": {},
   "source": [
    "#### Implement the joint likelihood method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd2186-7456-4517-9d72-df34a1644d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(w, X=XX, y=yy):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bd790-1af3-422a-9cda-60254cd6c86c",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "\n",
    "A Bayesian logistic regression model usually assumes a zero mean Gaussian prior for the weight parameter $\\mathbf{w}$:\n",
    "\n",
    "$$p(\\mathbf{w}|\\lambda) = \\mathcal{N}_m\\left(\\mathbf{w}; \\mathbf{0}, \\frac{1}{\\lambda}\\mathbf{I}\\right)=\\prod_{d=1}^m \\mathcal{N}(w_d; 0, 1/\\lambda)$$\n",
    "\n",
    "* where $\\mathcal{N}_m$ denotes a $m-$dimensional Gaussian distribution and where $\\lambda$ is the precision parameter of the Gaussian.\n",
    "\n",
    "#### Write down an expression for the log-transformed prior $\\ln p(\\mathbf{w}|\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54251063-e894-43a8-b434-eadf1ba93cb7",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e5312-14df-461b-87c1-77b3cf439fbf",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "\n",
    "Based on Bayes' rule, the unnormalised log posterior distribution is defined as\n",
    "\n",
    "$$\\ln p(\\mathbf{w}|\\{y^{(i)}\\}, \\mathbf{X}) = \\ln p(\\mathbf{w}|\\lambda) + \\ln p(\\{y^{(i)}\\}|\\mathbf{X}, \\mathbf{w}) +\\text{const}$$\n",
    "\n",
    "#### Write down an expression for the unnormalised log-transformed posterior:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ccff6-ba1e-4f24-b28f-dafcbc3f65d4",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224f820-af0f-4754-ab76-9d6aa26d135d",
   "metadata": {},
   "source": [
    "#### Implement the unnormalised log posterior below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6082e3-c0ae-48cb-b9e4-85b5ac69fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(w, X=XX, y=yy, lambda0=1e-2):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbebc0-9e1b-464d-822a-0b01aba6e85f",
   "metadata": {},
   "source": [
    "#### Report the unnormalised log posterior value at $\\mathbf{w} =\\mathbf{0}$ and $\\lambda = \\frac{1}{100}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7cc4d7-931f-4de7-8acb-72045e7f44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_posterior(np.zeros(XX.shape[1]), XX, yy, lambda0=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97319cd-3617-48d6-8901-4b9b86878089",
   "metadata": {},
   "source": [
    "### Task 2.4\n",
    "\n",
    "Implement a **Metropolis sampling** algorithm to draw samples from the unnormalised posterior:\n",
    "\n",
    "$$\\mathbf{w}^{(m)} \\sim p(\\mathbf{w}|\\{y^{(i)}\\},\\mathbf{X}),\\;\\; \\text{for }m = 1, \\ldots, M$$\n",
    "\n",
    "You should use the following proposal distribution\n",
    "\n",
    "$$q(\\mathbf{w}'|\\mathbf{w}) = \\mathcal{N}\\left (\\mathbf{w}, \\left(\\lambda \\mathbf{I} + \\frac{6}{\\pi^2} \\mathbf X^\\top \\mathbf X\\right)^{-1}\\right),$$\n",
    "\n",
    "You should convince yourself that the proposal distribution is symmetric. To draw random samples from multivariate Gaussian distribution, you may use `rng.multivariate_normal(mean, cov, size=800)`, where `mean` and `cov` are the corresponding mean and covariance parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d8b5e-5b0b-44e3-9fe7-79d73a1730d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_bayes_logistic_reg(X=XX, y=yy, lambda0=1e-2, mc=5000, burnin=1000):\n",
    "    return w_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c9e81-4933-4f90-92f6-87b5ba83d0ba",
   "metadata": {},
   "source": [
    "##### Implement a Monte Carlo prediction method and use the method to replicate the following decision boundary plot. Note that the Monte Carlo prediction is defined as:\n",
    "\n",
    "$$p(y_{test}|\\mathbf{x}_{test}) \\approx \\frac{1}{M} \\sum_{m=1}^M \\sigma(\\mathbf{x}_{test}, \\mathbf{w}^{(m)})$$\n",
    "\n",
    "You only need roughly $M=1000$ Monte Carlo samples to make accurate predictions.\n",
    "\n",
    "##### Explain why the Bayesian prediction should be preferred in comparison with the frequentist's MAP/ML estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7d417-2bd0-4f49-b875-762393b13882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_mc(xtest, wsamples):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3dbb33-916d-40e3-8000-ab882961ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replicate the plot below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071c665-dc70-47e9-ad44-061992ae97a5",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://leo.host.cs.st-andrews.ac.uk/figs/bayesian_preds.svg);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c461e-1b98-4884-83a9-3c531276fe62",
   "metadata": {},
   "source": [
    "### Task 2.5 (extension)\n",
    "\n",
    "Lastly, implement a **Metropolis-Hasting sampling** algorithm to draw samples from the unnormalised posterior:\n",
    "\n",
    "$$\\mathbf{w}^{(m)} \\sim p(\\mathbf{w}|\\{y^{(i)}\\},\\mathbf{X}),\\;\\; \\text{for }m = 1, \\ldots, M$$\n",
    "\n",
    "You should use the following proposal distribution\n",
    "\n",
    "$$q(\\mathbf{w}') = \\mathcal{N}\\left (\\mathbf{w}_{MAP}, \\left(10^2 \\cdot \\mathbf{I} + \\frac{6}{\\pi^2} \\mathbf X^\\top \\mathbf X\\right)^{-1}\\right),$$\n",
    "\n",
    "* where $\\mathbf{w}_{MAP}$ is the maximum aposteriori estimator (MAP), which can be estimated by gradient descent. Note that the proposal is no longer symmetric, therefore you need to use the Metropolis Hasting algorithm instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ea8e0-5f53-41bd-a45b-eb0c5e31a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hasting_bayes_logistic_reg(X=XX, y=yy, lambda0=1e-2, mc=5000, burnin=1000):\n",
    "    return w_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175da53a-64c0-442b-ad56-a758fdf0b9da",
   "metadata": {},
   "source": [
    "Reuse the prediction method above to replicate the prediction plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b10cb5",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Hand in via Moodle: the completed jupyter notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422044c",
   "metadata": {},
   "source": [
    "## Marking\n",
    "Your submission will be marked as a whole. \n",
    "\n",
    "* to get a grade above 13, you are expected to finish at least Task 1 to a good standard\n",
    "* to get a grade above 13 and up to 17, you are expected to answer Task 1 and Task 2.1-2.3 to a good standard\n",
    "* to achieve a grade of 17-18, you are expected to finish all tasks except Task 2.5 flawlessly \n",
    "* to get 18+, you are expected to attempt all questions flawlessly\n",
    "\n",
    "\n",
    "Marking is according to the standard mark descriptors published in the Student Handbook at:\n",
    "\n",
    "https://info.cs.st-andrews.ac.uk/student-handbook/learning-teaching/feedback.html#GeneralMarkDescriptors\n",
    "\n",
    "\n",
    "You must reference any external sources used. Guidelines for good academic practice are outlined in the student handbook at https://info.cs.st-andrews.ac.uk/student-handbook/academic/gap.html\n",
    "\n",
    "\n",
    "## Submission dates\n",
    "There are no fixed submission dates. Formal exam boards are held in January, May and September. Any provisional grades recorded on MMS are discussed by the Programme team and the External Examiner. Credits for a module can only be obtained after all the coursework has been discussed at an exam board, so students must submit work at least three weeks before the board date, giving time for grading and preparation of feedback.\n",
    "\n",
    "https://www.st-andrews.ac.uk/education/staff/assessment/reporting/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
